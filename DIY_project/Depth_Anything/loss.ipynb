{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T01:03:11.362731Z",
     "start_time": "2025-03-26T01:03:11.328021Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# 예시는 2x2 이미지, 배치 사이즈 2\n",
    "pred = torch.tensor([[[1,2],[4,1]],[[3,1],[4,1]]])\n",
    "gt = torch.tensor([[[2,3],[3,1]],[[2,1],[4,1]]])\n",
    "\n",
    "\n",
    "def loss_function(pred,y,cutMix=False,feature_alignment=False,f1=None,f2=None):\n",
    "    \"\"\"\n",
    "    :param pred: Prediction per pixel. size : BxHxW\n",
    "    :param y: Ground truth. size : BxHxW\n",
    "    :param perturbation: For unlabeled (pseudo) dataset\n",
    "    :param feature_alignment: For Idea comparison\n",
    "    :param f1,f2: latent vector from encoder that contains high representation per patch << 이거 논문에서는 그냥 loss 함수에 동일한 HW 썼지만은, 정황상 H W는 패치로 주는게 맞음 -> 이거 size [ B, num_P, embedding ]\n",
    "    :return:\n",
    "\n",
    "    \"\"\"\n",
    "    B,H,W = pred.shape\n",
    "\n",
    "    #pred = torch.reshape(pred,(B,H*W))\n",
    "    #y = torch.reshape(y,(B,H*W))\n",
    "\n",
    "    pred = pred.view(B, H * W)\n",
    "    y = y.view(B, H * W)\n",
    "    loss_l= 0\n",
    "    loss_u= 0\n",
    "    loss_feat = 0\n",
    "\n",
    "    def d_hat(d):\n",
    "        # 각 배치별 계산을 위해 dim 설정\n",
    "        # 여기서는 B x (HxW) 로 2차원이라고 가정\n",
    "        median, _ = torch.median(d, dim=-1) # 이러면 나오는 결과 : Bx1\n",
    "        # print(median)\n",
    "        t = median.unsqueeze(-1)\n",
    "        t = t.expand(-1,d.shape[-1])\n",
    "\n",
    "        #t = torch.matmul(median.unsqueeze(0),t)\n",
    "        s = torch.sum(torch.abs(d-t),dim=-1)/(d.shape[-1])\n",
    "        # s : Bx1 사이즈\n",
    "\n",
    "        s = s.unsqueeze(-1).expand(-1,d.shape[-1])\n",
    "\n",
    "        #print(\"d : \",d)\n",
    "        #print(\"t : \",t)\n",
    "        #print(\"s : \",s)\n",
    "        #print(\"분자 : \",torch.sum(d-t,dim=-1))\n",
    "\n",
    "        return (d-t)/s\n",
    "\n",
    "    def rho(pred,y):\n",
    "        return torch.abs(d_hat(pred) - d_hat(y))\n",
    "\n",
    "## 이미 배치 구성 단계에서 저걸 포함시켰어야한,.??\n",
    "    def cutmix(u_cutmix,u_normal_a,u_normal_b,mask):\n",
    "        \"\"\"\n",
    "        :param u_cutmix: cutmix된 이미지 -> student에게 먹일 데이터\n",
    "        :param u_normal: cutmix 안된 일반 이미지 -> teacher 에게 먹일 데이터\n",
    "        :param mask: u_whole과 u_part가 공유하는 부분만 1이고, 나머지는 0인 마스킹\n",
    "        :return: loss_u\n",
    "\n",
    "        < instruction >\n",
    "        \"\"\"\n",
    "\n",
    "        mask_flat = mask.view(B, H * W).float()\n",
    "        inv_mask = 1.0 - mask_flat\n",
    "\n",
    "        # 영역별 loss: 여기서는 rho를 이용하여 픽셀별 loss\n",
    "        ##### 이렇게 하면ㅇ ㅏ안돼 1!!!!! 왜ㅔ야냐하면 이미 우리 데이터 배치단위로 넣었다고 가정하잖아\n",
    "\n",
    "        loss_u_a = torch.mean(rho(pred * mask_flat, u_normal_a.view(B, H * W) * mask_flat), dim=-1)\n",
    "        loss_u_b = torch.mean(rho(pred * inv_mask, u_normal_b.view(B, H * W) * inv_mask), dim=-1)\n",
    "\n",
    "        return torch.mean(loss_u_a + loss_u_b)\n",
    "\n",
    "\n",
    "    if cutMix:\n",
    "\n",
    "        loss_u = cutmix(u_cutmix, u_normal_a, u_normal_b, mask)\n",
    "\n",
    "        if feature_alignment and f1 and f2 :\n",
    "            \"\"\"\n",
    "            f1 f2 간의 cos similarity 도입\n",
    "            여기서 f1는 student model의 encoder에서 나온 결과, f2는 frozen dino에서 나온 결과\n",
    "\n",
    "            고로 f1에는 perturbation이 적용되었음 -> 단 gaussian blur과 jittering같은 것만 포함\n",
    "\n",
    "            size [ B, num_P, embedding ]\n",
    "            \"\"\"\n",
    "            # f1 , f2 : B x P x embedding 사이즈\n",
    "            _ , num_patch, dim_embedding = f1.shape\n",
    "\n",
    "            dot_product = torch.sum(torch.mul(f1,f2),dim=-1) # Bx1 사이즈\n",
    "            size_f1 = torch.sqrt(torch.sum(torch.mul(f1,f1),dim=-1))\n",
    "            size_f2 = torch.sqrt(torch.sum(torch.mul(f2,f2),dim=-1))\n",
    "            denominator = torch.mul(size_f1,size_f2) # Bx1 사이즈\n",
    "\n",
    "            loss_feat = torch.sum(1-(dot_product/denominator)/num_patch)\n",
    "\n",
    "    else :\n",
    "        loss_l = torch.sum(rho(pred,y),dim=-1) / pred.shape[-1]\n",
    "\n",
    "    return loss_l + loss_u + loss_feat\n",
    "\n",
    "print(loss_function(pred,gt))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8333, 0.3000])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7e5bd78f3658b6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
